{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML model for Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dataset:\n",
    "df = pd.read_csv('../Dataset/Loan_default.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are No Missing value or outliers in this dataset, so there is no need for imputation and outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we can use binary encoding for binary categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding Binary Categorical Features\n",
    "Features = ['HasCoSigner', 'HasDependents', 'HasMortgage']\n",
    "\n",
    "for feature in Features:\n",
    "   df[feature] = df[feature].map({'Yes':1, 'No':0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop useless columns:\n",
    "df = df.drop(columns=['LoanID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For marital status and Loan Purpose, they dont have lot of difference in frequency, ordering is not important and their is more then one class so we have to use one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanPurpose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df,columns=['MaritalStatus','LoanPurpose'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education is an ordinal variable where order matters, so we use ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder(categories=[[\"High School\",\"Bachelor's\",\"Master's\",\"PhD\"]])\n",
    "df['Education'] = oe.fit_transform(df['Education'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some order to employment type, although it may not be significant. We can use Label Encoding for Employment Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EmploymentType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['EmploymentType'] = le.fit_transform(df['EmploymentType'])\n",
    "df['EmploymentType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-2: Feature Scaling for Numerical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age is continous variable present in patches/bins, so we first discretize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Number of bins from distribution of age\n",
    "discretizer = KBinsDiscretizer(n_bins=14, encode='ordinal', strategy='uniform')\n",
    "\n",
    "df['Age'] = discretizer.fit_transform(df[['Age']])\n",
    "df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'InterestRate', 'LoanTerm']\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MinMaxScaler = MinMaxScaler()\n",
    "df[num_features] = MinMaxScaler.fit_transform(df[num_features])\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Transformed Dataset an another dataset\n",
    "df = df.astype(float) # Convert all data to float\n",
    "# df.to_csv('../Dataset/Loan_default_transformed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Pipeline:\n",
    "Now we will create a pipeline which will automatically perform feature engineerring in the same way.\n",
    "\n",
    "> Note: We will still need to drop Loan ID manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,LabelEncoder, MinMaxScaler,  KBinsDiscretizer, FunctionTransformer\n",
    "\n",
    "## Defining feature classes\n",
    "binary_features = ['HasCoSigner', 'HasDependents','HasMortgage']\n",
    "ordinal_features = ['Education']\n",
    "nominal_features = ['MaritalStatus','LoanPurpose']\n",
    "label_encoded_features = ['EmploymentType']\n",
    "numerical_features = ['Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'InterestRate', 'LoanTerm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv('../Dataset/Loan_default.csv')\n",
    "DF = DF.drop(columns=['LoanID'])\n",
    "\n",
    "# Custom function for binary encoding\n",
    "def  BinaryEncoder(dataset):\n",
    "   for col in binary_features:\n",
    "      dataset[col] = dataset[col].map({'Yes': 1, 'No': 0})\n",
    "   return dataset\n",
    "\n",
    "# Custom function for label encoding\n",
    "def label_encode_columns(dataset):\n",
    "   return dataset.apply(lambda col: LabelEncoder().fit_transform(col) if col.dtype == 'O' else col)\n",
    "\n",
    "# Function Transformer for binary encoding\n",
    "FT_binary = FunctionTransformer(BinaryEncoder, validate=False, feature_names_out='one-to-one')\n",
    "FT_label = FunctionTransformer(label_encode_columns, validate=False, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom transformer for Age as it needs both discretization and scaling\n",
    "def bin_and_scale_age(dataset):\n",
    "   if dataset.ndim == 1: # Reshape into 2D\n",
    "      dataset = dataset.reshape(-1, 1)\n",
    "   \n",
    "   # Step-1: Discretization\n",
    "   kbins = KBinsDiscretizer(n_bins=14, encode='ordinal', strategy='uniform')\n",
    "   dataset = kbins.fit_transform(dataset)\n",
    "   \n",
    "   # Step-2: Scale the binned values\n",
    "   scaler = MinMaxScaler()\n",
    "   dataset = scaler.fit_transform(dataset)\n",
    "   \n",
    "   return dataset\n",
    "\n",
    "# Wrap the function in FunctionTransformer\n",
    "FT_age = FunctionTransformer(bin_and_scale_age, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Column Transformer\n",
    "Ct = ColumnTransformer(transformers=[\n",
    "   ('binary', FT_binary, binary_features),\n",
    "   ('ordinal', OrdinalEncoder(categories=[[\"High School\",\"Bachelor's\",\"Master's\",\"PhD\"]]), ordinal_features),\n",
    "   ('Nominal->OHE', OneHotEncoder(drop='first'), nominal_features),\n",
    "   ('Label->LE', FT_label, label_encoded_features),\n",
    "   ('Age Transformer', FT_age, ['Age']),\n",
    "   ('Numerical Scaling-> MinMax', MinMaxScaler(), numerical_features),\n",
    "]  ,remainder='passthrough',\n",
    "   force_int_remainder_cols = False, # This ensures column names remain correctly\n",
    ")\n",
    "Ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "   ('Preprocessor',Ct)\n",
    "])\n",
    "\n",
    "# Apply transformation\n",
    "df_processed = pipe.fit_transform(DF)\n",
    "display(df_processed,df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_column_names(ct, original_features):\n",
    "   \"\"\"Extract transformed column names from ColumnTransformer.\"\"\"\n",
    "   output_features = []\n",
    "   \n",
    "   for name, transformer, features in ct.transformers_:\n",
    "      if transformer == 'passthrough':\n",
    "         output_features.extend(features)\n",
    "      elif transformer == 'drop':\n",
    "         continue\n",
    "      elif hasattr(transformer, \"get_feature_names_out\"):\n",
    "         output_features.extend(transformer.get_feature_names_out(features))\n",
    "      else:\n",
    "         # Handle FunctionTransformer case\n",
    "         output_features.extend([f\"{name}_{i}\" for i in range(len(features))])\n",
    "   \n",
    "   return output_features\n",
    "\n",
    "# Extract correct feature names\n",
    "transformed_column_names = get_transformed_column_names(pipe.named_steps[\"Preprocessor\"], DF.columns)\n",
    "\n",
    "# Convert transformed data back to DataFrame\n",
    "df_processed = pd.DataFrame(df_processed, columns=transformed_column_names)\n",
    "df_processed.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Rename correctly:\n",
    "# names = {'binary_0':'HasCoSigner', \"binary_1\":'HasDependents', \"binary_2\":'HasMortgage',\n",
    "#    'Label->LE_0':'EmploymentType', \"Age Transformer_0\":'Age',}\n",
    "# df_processed = df_processed.rename(columns=names)\n",
    "# df_processed.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Loan Prediction)",
   "language": "python",
   "name": "virtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
